{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2243895,"sourceType":"datasetVersion","datasetId":1347338}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Handwritten Text Recognition with Deep Learning\n\nThis notebook explores the development of a Handwritten Text Recognition (HTR) service using deep learning techniques. The service aims to accurately transcribe handwritten text from images, enabling various applications such as document processing, form automation, and data extraction.\n\n**Inspiration and Architecture:**\n\nThe HTR service draws inspiration from the advanced architecture presented in the Keras Handwriting Recognition example ([source](https://keras.io/examples/vision/handwriting_recognition/)). This architecture, rooted in deep learning principles, is meticulously designed to recognize and transcribe handwritten text with high accuracy and efficiency. The underlying model has been further customized and optimized for seamless deployment in various settings, ensuring its suitability for real-world use cases.\n\nThis notebook delves into the step-by-step process of building, training, and evaluating the HTR model. We'll explore essential components like data preparation, model architecture design, training techniques, and performance evaluation metrics. By following along, you'll gain a comprehensive understanding of how deep learning can be harnessed for effective handwritten text recognition.\n\n**If you think this notebook could be a resource for others, consider giving it an upvote for better discoverability!**\n\n## Online Demo\n\nYou can work with online demo in the following address: https://hamiddamadi.ir/app/textRecognition.\n\n## Essential Libraries for Handwritten Text Recognition\n\nThis cell imports several essential libraries for building and manipulating the handwritten text recognition (HTR) model:\n\n* **`os`:** Provides operating system functionalities, potentially useful for tasks like:\n    * File path manipulation during data loading from the file system.\n    * Saving and managing model checkpoints during training.\n* **`numpy`:** Offers powerful array manipulation and mathematical operations, crucial for various tasks like:\n    * Data pre-processing, such as image resizing and normalization.\n    * Representing and manipulating image data as NumPy arrays.\n    * Performing calculations within the model, such as matrix multiplications.\n* **`tensorflow`:** Serves as the core deep learning framework for building and training the HTR model. It provides tools for:\n    * Defining the model architecture, including layers and their connections.\n    * Performing computations on tensors (multidimensional data arrays) during training and inference.\n    * Optimizing the model's performance through techniques like backpropagation.\n* **`sklearn.model_selection`:** Offers functionalities for splitting data into training and testing sets. This is essential for:\n    * Evaluating the model's performance on unseen data during training.\n    * Ensuring the model generalizes well to new handwritten text samples.\n* **`matplotlib.pyplot`:** Enables data visualization using plots and charts. This can be helpful for:\n    * Visualizing data distributions, such as the distribution of characters in the training data.\n    * Understanding training progress, such as plotting the model's loss over training epochs.\n    * Analyzing results, such as visualizing predicted vs. ground truth text for evaluation purposes.\n\nThese libraries will be utilized throughout the notebook, playing crucial roles in building, training, and evaluating the HTR model to effectively recognize handwritten text.","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell defines various constants and hyperparameters that will be used throughout the notebook for the handwritten text recognition (HTR) model:\n\n* **`IMAGE_SIZE`:** This tuple represents the fixed size to which all input images will be resized. In this case, images will be resized to 128 pixels wide and 32 pixels high. This ensures consistency in the input data and simplifies processing within the model.\n* **`BATCH_SIZE`:** This integer specifies the number of images processed by the model in each training iteration (epoch). A batch size of 64 means the model will update its weights based on the gradients calculated from 64 images at a time. Choosing an appropriate batch size can impact training speed, memory usage, and convergence.\n* **`EPOCHS`:** This integer represents the number of times the entire training dataset will be passed through the model during training. In this case, the model will be trained for 10 epochs. This hyperparameter fine-tunes the model's learning process and influences its ability to learn patterns from the data.\n* **`PADDING_TOKEN`:** This integer represents a special token used for padding sequences during text processing. It is often used to ensure all sequences have the same length, which is necessary for certain model architectures. A value of 99 is assigned as the padding token in this case.\n\nThese constants and hyperparameters will be used in various parts of the notebook, including data pre-processing, model building, and training. Carefully choosing and adjusting these values can significantly impact the model's performance and effectiveness in recognizing handwritten text.\n","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = (128, 32)\nBATCH_SIZE = 64\nEPOCHS = 50\nPADDING_TOKEN = 99","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Creating Datasets\n\n### Preprocessing the Handwritten Text Dataset\n\nThis cell defines and executes the `preprocess_dataset` function, which is responsible for loading and processing the handwritten text dataset:\n\n**Data Path:**\n\n* **`DATA_INPUT_PATH`:** This variable stores the path to the dataset, assumed to be located in the `/kaggle/input/iam-handwriting-word-database` directory. This path needs to be adjusted if the data is located elsewhere.\n\n**Function Details:**\n\n1. **Open the labels file:** The function opens the `words.txt` file located within the data directory. This file contains information about each image and its corresponding label (the handwritten text).\n2. **Iterate over lines:** It iterates through each line in the file, skipping comments and empty lines.\n3. **Extract information:** \n    * **Word ID:** Identifies the unique identifier for the word.\n    * **Image filename:** Constructs the filename for the corresponding image based on the word ID.\n    * **Label:** Extracts the handwritten text associated with the image.\n4. **Process image and label:** \n    * **Image path:** Constructs the full path to the image file using the data directory and filename.\n    * **Check image existence:** Verifies if the image file exists and has a non-zero size to avoid processing non-existent or empty images.\n    * **Add to lists:** \n        * Appends the image path to the `images_path` list.\n        * Appends the label to the `labels` list.\n        * Adds each unique character from the label to the `characters` set.\n    * **Update max length:** Tracks the maximum length (number of characters) across all labels to be used later for padding.\n\n**Character Mapping:**\n\n* After processing all lines, the function:\n    * Sorts the unique characters encountered in the labels to create a consistent order.\n    * Defines two `StringLookup` layers from TensorFlow Keras:\n        * `char_to_num`: Maps each unique character to a unique integer, enabling efficient processing within the model.\n        * `num_to_char`: Maps the integer back to the original character, useful for decoding predictions later.\n\n**Running the Function:**\n\n* The `preprocess_dataset` function is called at the end of the cell to initiate the data preprocessing process.\n\n**Printing Information:**\n\n* The function prints the following information after processing the data:\n    * List of unique characters (`characters`)\n    * Maximum length of any label (`max_len`)\n\nThis preprocessed data, including the image paths, labels, character mapping, and maximum sequence length, will be used for subsequent steps like loading images, preparing sequences, and building the HTR model.\n","metadata":{}},{"cell_type":"code","source":"DATA_INPUT_PATH = \"/kaggle/input/iam-handwriting-word-database\"\n\nimages_path = []\nlabels = []\n\ndef preprocess_dataset():\n    characters = set()\n    max_len = 0\n    with open(os.path.join(DATA_INPUT_PATH, 'iam_words', 'words.txt'), 'r') as file:\n        lines = file.readlines()\n\n        for line_number, line in enumerate(lines):\n            # Skip comments and empty lines\n            if line.startswith('#') or line.strip() == '':\n                continue\n\n            # Split the line and extract information\n            parts = line.strip().split()\n\n            # Continue with the rest of the code\n            word_id = parts[0]\n\n            first_folder = word_id.split(\"-\")[0]\n            second_folder = first_folder + '-' + word_id.split(\"-\")[1]\n\n            # Construct the image filename\n            image_filename = f\"{word_id}.png\"\n            image_path = os.path.join(\n                DATA_INPUT_PATH, 'iam_words', 'words', first_folder, second_folder, image_filename)\n\n            # Check if the image file exists\n            if os.path.isfile(image_path) and os.path.getsize(image_path):\n\n                images_path.append(image_path)\n\n                # Extract labels\n                label = parts[-1].strip()\n                for char in label:\n                    characters.add(char)\n\n                max_len = max(max_len, len(label))\n                labels.append(label)\n\n    characters = sorted(list(characters))\n\n    print('characters: ', characters)\n    print('max_len: ', max_len)\n    # Mapping characters to integers.\n    char_to_num = tf.keras.layers.StringLookup(\n        vocabulary=list(characters), mask_token=None)\n\n    # Mapping integers back to original characters.\n    num_to_char = tf.keras.layers.StringLookup(\n        vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n    )\n    return characters, char_to_num, num_to_char, max_len\n    \ncharacters, char_to_num, num_to_char, max_len = preprocess_dataset()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocessing Functions for Images and Labels\n\nThis cell defines three functions used for pre-processing images and labels:\n\n**1. `distortion_free_resize`:**\n\n* This function resizes an image while preserving its aspect ratio using TensorFlow's `tf.image.resize` function with `preserve_aspect_ratio=True`.\n* It then calculates the amount of padding needed to make the image size match the target size defined by `img_size` (a tuple of width and height).\n* The function ensures equal padding on both top/bottom and left/right sides for consistency.\n* Finally, it performs the following operations:\n    * Pads the image with the calculated padding amounts.\n    * Transposes the image dimensions.\n    * Flips the image horizontally (optional, can be helpful for data augmentation).\n\n**2. `preprocess_image`:**\n\n* This function takes an image path and the target image size (`img_size`) as input.\n* It reads the image file using `tf.io.read_file`.\n* Decodes the PNG image into a grayscale image using `tf.image.decode_png` with a channel dimension of 1.\n* Calls the `distortion_free_resize` function to resize and pad the image.\n* Normalizes the image pixel values to the range [0, 1] by dividing by 255.0.\n* Returns the preprocessed image as a floating-point tensor.\n\n**3. `vectorize_label`:**\n\n* This function takes a label string (the handwritten text) as input.\n* It maps each character in the label to its corresponding integer using the `char_to_num` StringLookup layer defined earlier.\n* It calculates the length of the label sequence.\n* It pads the label sequence with the `PADDING_TOKEN` (value of 99) up to the maximum length (`max_len`) defined earlier. This ensures all labels have the same length, which is necessary for certain model architectures.\n* Returns the padded label sequence as a tensor of integers.\n\nThese functions will be instrumental in transforming the raw image data and labels into a format suitable for training the HTR model. The preprocessed images and labels will be used in the next steps for data loading and model building.","metadata":{}},{"cell_type":"code","source":"def distortion_free_resize(image, img_size):\n    w, h = img_size\n    image = tf.image.resize(image, size=(h, w), preserve_aspect_ratio=True)\n\n    # Check tha amount of padding needed to be done.\n    pad_height = h - tf.shape(image)[0]\n    pad_width = w - tf.shape(image)[1]\n\n    # Only necessary if you want to do same amount of padding on both sides.\n    if pad_height % 2 != 0:\n        height = pad_height // 2\n        pad_height_top = height + 1\n        pad_height_bottom = height\n    else:\n        pad_height_top = pad_height_bottom = pad_height // 2\n\n    if pad_width % 2 != 0:\n        width = pad_width // 2\n        pad_width_left = width + 1\n        pad_width_right = width\n    else:\n        pad_width_left = pad_width_right = pad_width // 2\n\n    image = tf.pad(\n        image,\n        paddings=[\n            [pad_height_top, pad_height_bottom],\n            [pad_width_left, pad_width_right],\n            [0, 0],\n        ],\n    )\n\n    image = tf.transpose(image, perm=[1, 0, 2])\n    image = tf.image.flip_left_right(image)\n    return image\n\ndef preprocess_image(image_path, img_size):\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_png(image, 1)\n    image = distortion_free_resize(image, img_size)\n    image = tf.cast(image, tf.float32) / 255.0\n    return image\n\ndef vectorize_label(label):\n    label = char_to_num(tf.strings.unicode_split(\n        label, input_encoding=\"UTF-8\"))\n    length = tf.shape(label)[0]\n    pad_amount = max_len - length\n    label = tf.pad(label, paddings=[[0, pad_amount]],\n                   constant_values=PADDING_TOKEN)\n    return label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preparing the Handwritten Text Dataset\n\nThis cell defines two functions to prepare the dataset for training the HTR model:\n\n**1. `process_images_labels`:**\n\n* This function takes an image path and its corresponding label as input.\n* It utilizes the previously defined functions:\n    * `preprocess_image`: Reads, resizes, normalizes, and preprocesses the image.\n    * `vectorize_label`: Converts the label string into a padded sequence of integers.\n* It returns a dictionary containing the preprocessed image and label.\n\n**2. `prepare_dataset`:**\n\n* This function takes lists of image paths and labels as input.\n* It creates a TensorFlow dataset using `tf.data.Dataset.from_tensor_slices`.\n* It applies the `process_images_labels` function to each element (image path, label) in the dataset using parallel processing (controlled by `num_parallel_calls=AUTOTUNE`). This improves efficiency by utilizing multiple CPU cores.\n* It uses the following techniques for further optimization:\n    * **Batching:** Groups elements into batches of size `BATCH_SIZE` (defined earlier) for efficient processing during training.\n    * **Caching:** Stores the preprocessed data in memory to avoid redundant processing on subsequent epochs.\n    * **Prefetching:** Overlaps data preprocessing with training to improve training speed.\n\n**Data Size Information:**\n\n* The function prints the lengths of the `image_paths` and `labels` lists to verify dataset size.\n\n**Output:**\n\n* This function returns the prepared dataset, which is now ready to be used for training the HTR model.\n\nBy preparing the dataset in this way, we ensure that the images and labels are efficiently processed and fed into the model during training, leading to improved performance and faster training times.","metadata":{}},{"cell_type":"code","source":"def process_images_labels(image_path, label):\n    image = preprocess_image(image_path, IMAGE_SIZE)\n    label = vectorize_label(label)\n    return {\"image\": image, \"label\": label}\n\ndef prepare_dataset(image_paths, labels):\n    AUTOTUNE = tf.data.AUTOTUNE\n    print('len(image_paths): ', len(image_paths))\n    print('len(labels): ', len(labels))\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels)).map(\n        process_images_labels, num_parallel_calls=AUTOTUNE\n    )\n    return dataset.batch(BATCH_SIZE).cache().prefetch(AUTOTUNE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Splitting the Dataset\n\nThis cell defines a function `split_dataset` and calls it to split the prepared dataset into training, validation, and test sets:\n\n**1. `split_dataset` function:**\n\n* This function performs the following steps:\n    * **Initial split:** Splits the entire dataset (image paths and labels) into training and testing sets using `sklearn.model_selection.train_test_split`. The `test_size` parameter is set to 0.2, allocating 20% of the data for the testing set. The `random_state` parameter is set to 42 for reproducibility.\n    * **Further split:** Splits the test set further into validation and final test sets using another `train_test_split` call. This time, half of the remaining data (10% of the original dataset) is allocated for validation.\n* **Prepare datasets:** Uses the `prepare_dataset` function defined earlier to prepare separate datasets for training, validation, and testing.\n\n**2. Splitting and preparing:**\n\n* The function calls itself (using `split_dataset()`) to execute the splitting and preparation steps.\n* The function assigns the resulting training, validation, and test sets to the respective variables:\n    * `train_set`: Contains the training data for training the model.\n    * `val_set`: Contains the validation data for monitoring model performance during training.\n    * `test_set`: Contains the final test data for evaluating the model's generalization ability on unseen data.\n\n**Output:**\n\n* This cell does not explicitly print any output, but it assigns the split and prepared datasets to variables for use in subsequent parts of the notebook, namely model building and training.\n\nBy splitting the data into separate training, validation, and test sets, we:\n\n* Ensure the model is trained on unseen data during validation, allowing for unbiased evaluation of its performance.\n* Reserve a portion of the data for final testing to assess the model's ability to generalize to completely new handwritten text samples.","metadata":{}},{"cell_type":"code","source":"def split_dataset():\n    # Split the data into training, validation, and test sets using train_test_split\n    train_images, test_images, train_labels, test_labels = train_test_split(\n        images_path, labels, test_size=0.2, random_state=42\n    )\n\n    # Further split the test set into validation and final test sets\n    val_images, test_images, val_labels, test_labels = train_test_split(\n        test_images, test_labels, test_size=0.5, random_state=42\n    )\n\n    train_set = prepare_dataset(train_images, train_labels)\n    val_set = prepare_dataset(val_images, val_labels)\n    test_set = prepare_dataset(test_images, test_labels)\n    \n    return train_set, val_set, test_set\n\ntrain_set, val_set, test_set = split_dataset()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the Handwritten Text Recognition Model (HTR)\n\n### Custom CTC Layer\n\nThis code defines a custom layer named `CTCLayer` that inherits from `tf.keras.layers.Layer`. This layer is specifically designed for handling the Connectionist Temporal Classification (CTC) loss function commonly used in sequence recognition tasks like handwritten text recognition (HTR).\n\n**Initialization:**\n\n* The `__init__` method initializes the layer and sets the internal loss function to `tf.keras.backend.ctc_batch_cost`. This function calculates the CTC loss, which is essential for training the HTR model.\n\n**Call method:**\n\n* This method defines the forward pass of the layer, taking the ground truth labels (`y_true`) and model predictions (`y_pred`) as input.\n* It performs the following steps:\n    * **Calculate shapes:** Extracts relevant dimensions from the input shapes:\n        * Batch size (`batch_len`) from the number of samples in the ground truth labels.\n        * Predicted sequence length (`input_length`) from the shape of the predictions.\n        * Ground truth label length (`label_length`) from the shape of the labels.\n    * **Reshape lengths:** Reshapes the sequence lengths to have a batch dimension (size of batch x 1) for compatibility with the CTC loss function.\n    * **Calculate loss:** Calculates the CTC loss using the internal `loss_fn` and adds it to the model's total loss using `self.add_loss`.\n    * **Return predictions:** During testing (when `y_true` is None), the layer simply returns the predictions without calculating the loss.\n\n**Key Points:**\n\n* This custom layer simplifies the integration of the CTC loss into the HTR model by encapsulating the loss calculation logic within the layer itself.\n* The layer handles reshaping the sequence lengths to the appropriate format required by the CTC loss function.\n* It also provides flexibility by returning the predictions during testing, allowing for decoding and evaluation without calculating the loss.\n\nBy incorporating this layer into the model, we can effectively train the HTR model using the CTC loss function, enabling it to learn to recognize handwritten text sequences.","metadata":{}},{"cell_type":"code","source":"class CTCLayer(tf.keras.layers.Layer):\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = tf.keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * \\\n            tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * \\\n            tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions.\n        return y_pred","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This cell defines and builds the HTR model using TensorFlow Keras. Here's a breakdown of the code:\n\n**Model Inputs:**\n\n* **Image input:** The model takes an image as input using a `tf.keras.Input` layer named \"image\". The input shape is defined as `(IMAGE_SIZE[0], IMAGE_SIZE[1], 1)`, where:\n    * `IMAGE_SIZE[0]`: Height of the image (128 pixels in this case).\n    * `IMAGE_SIZE[1]`: Width of the image (32 pixels in this case).\n    * The final dimension of 1 indicates a grayscale channel.\n* **Label input:** The model also takes the corresponding label (the handwritten text) as input using another `tf.keras.Input` layer named \"label\". This layer has a shape of `(None,)`, indicating a sequence of characters with variable length.\n\n**Model Architecture:**\n\nThe model follows a sequence of convolutional and recurrent layers:\n\n* **Convolutional layers:**\n    * Two convolutional layers with 3x3 kernels and ReLU activation are used to extract features from the input image.\n    * Max pooling layers are used after each convolutional layer to reduce the dimensionality and introduce translation invariance.\n* **Reshaping layer:**\n    * The output of the second convolutional layer is reshaped to prepare it for the recurrent layers.\n    * The new shape is calculated based on the image size and the number of filters in the previous layer.\n* **Dense layer:**\n    * A dense layer with 64 units and ReLU activation is added for further feature extraction.\n* **Dropout layer:**\n    * A dropout layer with a rate of 0.2 is used to prevent overfitting.\n* **Bidirectional LSTMs:**\n    * Two bidirectional LSTMs with 128 and 64 units are used, respectively.\n        * Bidirectional LSTMs process the sequence in both directions, capturing dependencies from both past and future elements in the sequence.\n        * Return sequences are set to `True` to allow processing the entire sequence at once.\n        * Dropout is applied to each LSTM layer (with a rate of 0.25) to further prevent overfitting.\n* **Output layer:**\n    * A dense layer with a number of units equal to the vocabulary size (number of unique characters) plus 2 is used.\n        * The extra 2 units represent the padding token (used for sequences with different lengths) and the blank character (used in the CTC loss function).\n    * The output layer uses the softmax activation function to predict the probability distribution over the characters for each timestep in the sequence.\n\n**CTC Layer:**\n\n* A custom `CTCLayer` defined earlier is used as the final layer.\n    * It takes the ground truth labels and the model predictions as input.\n    * It calculates the CTC loss and adds it to the model's total loss.\n    * During testing, it simply returns the predictions.\n\n**Model Compilation:**\n\n* The model is compiled using the Adam optimizer with a learning rate of 0.001.\n* A summary of the model architecture is printed using `model.summary()`.\n\n**Calling the Function:**\n\n* The `build_model` function is called, which defines and compiles the HTR model.\n\nThis model architecture utilizes convolutional layers to extract features from the images and recurrent layers (LSTMs) to learn the temporal dependencies between characters in the sequence. The CTC loss function is used during training to guide the model towards learning accurate representations of handwritten text sequences.\n\nBy building and compiling this model, we are now ready to train it on the prepared dataset using the CTC loss function, enabling it to learn and recognize handwritten text.","metadata":{}},{"cell_type":"code","source":"def build_model():\n    input_img = tf.keras.Input(\n        shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1), name=\"image\")\n    labels = tf.keras.layers.Input(name=\"label\", shape=(None,))\n\n    x = tf.keras.layers.Conv2D(\n        32,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n    )(input_img)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    x = tf.keras.layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n    )(x)\n    x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n    new_shape = ((IMAGE_SIZE[0] // 4), (IMAGE_SIZE[1] // 4) * 64)\n    x = tf.keras.layers.Reshape(target_shape=new_shape)(x)\n    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.25)\n    )(x)\n    x = tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(64, return_sequences=True, dropout=0.25)\n    )(x)\n    x = tf.keras.layers.Dense(\n        len(char_to_num.get_vocabulary()) + 2, activation=\"softmax\", name=\"dense2\"\n    )(x)\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n    model = tf.keras.models.Model(\n        inputs=[input_img, labels], outputs=output\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n    model.summary()\n    return model\n    \nmodel = build_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Edit Distance Callback Class\n\nThis code defines a custom callback class named `EditDistanceCallback` that inherits from `tf.keras.callbacks.Callback`. This callback calculates and monitors the edit distance between predicted and ground truth labels during training:\n\n**Initialization:**\n\n* Takes the following arguments in its constructor:\n    * `pred_model`: The model used for making predictions (typically the built HTR model).\n    * `max_len`: The maximum length of the label sequences.\n    * `validation_images`: Validation images used for calculating edit distance.\n    * `validation_labels`: Corresponding validation labels for the images.\n\n**`calculate_edit_distance` Method:**\n\n* This method calculates the average edit distance between a batch of predictions and their corresponding ground truth labels.\n* It performs the following steps:\n    * Converts the ground truth labels to sparse tensors.\n    * Makes predictions using the provided model and converts them to sparse tensors.\n    * Calculates the edit distance between each predicted sequence and its corresponding label using the `tf.edit_distance` function.\n    * Calculates and returns the average edit distance across the entire batch.\n\n**`on_epoch_end` Method:**\n\n* This method is called at the end of each training epoch.\n* It iterates through the validation images and labels:\n    * For each image-label pair:\n        * Makes predictions using the `prediction_model`.\n        * Calculates the edit distance using the `calculate_edit_distance` method.\n    * Calculates the mean edit distance across all validation samples for the current epoch.\n    * Prints the mean edit distance for the current epoch.\n\n**Purpose:**\n\n* This callback provides a way to monitor the model's performance on the validation set in terms of edit distance, which reflects the number of edits (insertions, deletions, substitutions) required to transform the predicted sequence into the ground truth label.\n* By tracking the edit distance over time, we can gain insights into the model's ability to learn and recognize handwritten text accurately.\n\nBy incorporating this callback during training, we can monitor the model's progress beyond just the training and validation losses, providing a more comprehensive understanding of its performance on unseen data.","metadata":{}},{"cell_type":"code","source":"class EditDistanceCallback(tf.keras.callbacks.Callback):\n    def __init__(self, pred_model, max_len, validation_images, validation_labels):\n        super().__init__()\n        self.prediction_model = pred_model\n        self.max_len = max_len\n        self.validation_images = validation_images\n        self.validation_labels = validation_labels\n\n    def calculate_edit_distance(self, labels, predictions, max_len):\n        # Get a single batch and convert its labels to sparse tensors.\n        saprse_labels = tf.cast(tf.sparse.from_dense(labels), dtype=tf.int64)\n\n        # Make predictions and convert them to sparse tensors.\n        input_len = np.ones(predictions.shape[0]) * predictions.shape[1]\n        predictions_decoded = tf.keras.backend.ctc_decode(\n            predictions, input_length=input_len, greedy=True\n        )[0][0][:, :max_len]\n        sparse_predictions = tf.cast(\n            tf.sparse.from_dense(predictions_decoded), dtype=tf.int64\n        )\n\n        # Compute individual edit distances and average them out.\n        edit_distances = tf.edit_distance(\n            sparse_predictions, saprse_labels, normalize=False\n        )\n        return tf.reduce_mean(edit_distances)\n\n    def on_epoch_end(self, epoch, logs=None):\n        edit_distances = []\n\n        for i in range(len(self.validation_images)):\n            labels = self.validation_labels[i]\n            predictions = self.prediction_model.predict(\n                self.validation_images[i])\n            edit_distances.append(self.calculate_edit_distance(\n                labels, predictions, self.max_len).numpy())\n\n        print(\n            f\"Mean edit distance for epoch {epoch + 1}: {np.mean(edit_distances):.4f}\"\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Handwritten Text Recognition (HTR) Model\n\nThis code block defines and executes the training process for the HTR model:\n\n**Preparing Validation Data:**\n\n* Retrieves validation images and labels from the `val_set` dataset and stores them in separate lists for later use in the custom callback.\n\n**Creating a Prediction Model:**\n\n* Creates a new model, `prediction_model`, using the layers of the original model needed for inference. This model accepts images as input and outputs the character probabilities before the CTC layer.\n* Extracts these layers using `model.get_layer(name=\"image\").input` and `model.get_layer(name=\"dense2\").output`.\n\n**Setting Up Callbacks:**\n\n* **EditDistanceCallback:** \n    * An instance of the custom `EditDistanceCallback` class is created, providing the prediction model, maximum label length, validation images, and labels.\n    * This callback monitors the edit distance on the validation set during training.\n* **EarlyStopping:** \n    * A standard `EarlyStopping` callback is instantiated to monitor validation loss and stop training if it doesn't improve for 10 epochs, restoring the best weights found so far.\n\n**Training the Model:**\n\n* The `model.fit` function initiates the training process with the following parameters:\n    * `train_set`: The training dataset for model training.\n    * `validation_data=val_set`: The validation dataset for evaluating performance during training.\n    * `epochs=EPOCHS`: The maximum number of epochs to train (unless interrupted by early stopping).\n    * `callbacks=[edit_distance_callback, early_stopping]`: The specified callbacks to monitor training and potentially stop it early.\n\n**Return Value:**\n\n* The function returns the training history object `hist`, containing information about the training process, such as losses and metrics for each epoch.\n\n**Execution:**\n\n* The `history` variable is assigned the result of calling the `train_model` function, initiating the model training process with the specified settings and callbacks.","metadata":{}},{"cell_type":"code","source":"def train_model():\n\n    validation_images = []\n    validation_labels = []\n\n    for batch in val_set:\n        validation_images.append(batch[\"image\"])\n        validation_labels.append(batch[\"label\"])\n\n    prediction_model = tf.keras.models.Model(\n        model.get_layer(name=\"image\").input, model.get_layer(\n            name=\"dense2\").output\n    )\n    edit_distance_callback = EditDistanceCallback(\n        prediction_model, max_len, validation_images, validation_labels)\n    early_stopping = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\", patience=10, restore_best_weights=True\n    )\n    # Train the model.\n    hist = model.fit(\n        train_set,\n        validation_data=val_set,\n        epochs=EPOCHS,\n        callbacks=[edit_distance_callback, early_stopping],\n    )\n    return hist, prediction_model\n\nhistory, prediction_model = train_model()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing the Training History\n\nThis code block defines a function `visualize_train_history` and calls it to visualize the training history stored in the `history` object:\n\n**Function Definition:**\n\n* The function takes the `history` object returned from the training process as input.\n* It creates a Matplotlib figure with a specific size (12 inches wide and 4 inches high).\n\n**Plotting Accuracy and Loss:**\n\n* The function uses subplots to create a one-panel figure:\n    * **Loss:**\n        * Plots the training and validation loss values retrieved from the `history` object using `history.history['loss']` and `history.history['val_loss']`.\n        * Labels the axes and adds a legend similar to the accuracy plot.\n\n* The `plt.tight_layout()` function adjusts the spacing between subplots for better readability.\n* Finally, `plt.show()` displays the generated plot.\n\n**Calling the Function:**\n\n* The `visualize_train_history` function is called, passing the `history` object obtained from the training process. This triggers the creation and display of the visualization, allowing you to visually inspect the model's performance during training.\n\nBy visualizing the training history, you can gain valuable insights into the model's learning behavior. \n* The loss plots show how the model's loss (a measure of how well it fits the training data) decreases as training progresses. \n* Ideally, the training accuracy should increase and the training loss should decrease over time, while the validation accuracy and loss should also improve or at least not significantly worsen, indicating that the model is generalizing well to unseen data.","metadata":{}},{"cell_type":"code","source":"def visualize_train_history(history):\n    plt.figure(figsize=(12, 4))\n\n    # Plot training & validation loss values\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n\n    plt.tight_layout()\n    plt.show()\n\nvisualize_train_history(history)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluating the HTR Model on the Test Set\n\nThis code block defines and executes the evaluation process for the trained HTR model on the unseen test set:\n\n**Function Definition:**\n\n* The `evaluate_model` function calculates and prints the accuracy of the model on the test set.\n\n**Evaluation:**\n\n* The `model.evaluate` method is called with the `test_set` dataset as input. This performs evaluation on the test data and returns a list of metrics, including accuracy.\n* The first element of the returned list, corresponding to the model's accuracy on the test set, is extracted and stored in the `accuracy` variable.\n* The accuracy value is then printed with a descriptive message (\"Test Accuracy:\").\n\n**Execution:**\n\n* The `evaluate_model` function is called, which triggers the evaluation process and prints the test accuracy for the trained model.\n\n**Interpretation:**\n\n* The test accuracy metric reflects how well the model generalizes to unseen data not encountered during training. Ideally, the test accuracy should be comparable to the validation accuracy, indicating that the model has learned robust features and can perform well on new handwritten text samples.\n\nBy evaluating the model on the test set, you can gauge its ability to recognize handwritten text beyond the data it was trained on, providing valuable insights into its real-world applicability.","metadata":{}},{"cell_type":"code","source":"def evaluate_model():\n    accuracy = model.evaluate(test_set)\n    print(\"Test Accuracy:\", accuracy)\n\nevaluate_model() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Trained HTR Model\n\nThis code block defines and calls a function to save the trained HTR model:\n\n**Function Definition:**\n\n* The `save_model` function is defined to save the trained model to disk.\n\n**Saving the Model:**\n\n* It creates the directory specified in `MODEL_OUTPUT_PATH` if it doesn't exist using `os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)`.\n* It constructs the full file path for the model by combining the `MODEL_OUTPUT_PATH` and the model name (`MODEL_NAME`) with the `.keras` extension using `os.path.join`.\n* Finally, it calls `prediction_model.save` to save the model to the constructed file path.\n\n**Execution:**\n\n* The `save_model` function is called, which triggers the saving process. This preserves the trained model, allowing you to load it later for inference or fine-tuning on new data.\n\n**Key Points:**\n\n* The model is saved in the Keras format, which allows for easy loading and use in future applications.\n* The directory structure and naming convention (`MODEL_OUTPUT_PATH` and `MODEL_NAME`) can be adjusted to your preference and project requirements.\n\nBy saving the trained model, you can:\n\n* Reuse it for making predictions on new handwritten text samples without retraining the entire model.\n* Share the model with others for further evaluation or integration into applications.\n* Continue training the model on additional data in the future if needed.","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'MODEL_NAME'\nMODEL_OUTPUT_PATH = '/kaggle/working/'\n        \ndef save_model():\n    \"\"\"\n    Save the trained HTR model.\n    \"\"\"\n    os.makedirs(MODEL_OUTPUT_PATH, exist_ok=True)\n    prediction_model.save(os.path.join(\n        MODEL_OUTPUT_PATH, f'{MODEL_NAME}.keras'))\n    \nsave_model()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search.\n    results = tf.keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n        :, :max_len\n    ]\n\n    # Iterate over the results and get back the text.\n    output_text = []\n    for res in results:\n        res = tf.gather(res, tf.where(tf.math.not_equal(res, -1)))\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text\n\n\n#  Let's check results on some test samples.\nfor batch in test_set.take(2):\n    batch_images = batch[\"image\"]\n    batch_labels = batch[\"label\"]\n    _, ax = plt.subplots(4, 4, figsize=(15, 8))\n\n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    for i in range(16):\n        img = batch_images[i]\n        img = tf.image.flip_left_right(img)\n        img = tf.transpose(img, perm=[1, 0, 2])\n        img = (img * 255.0).numpy().clip(0, 255).astype(np.uint8)\n        img = img[:, :, 0]\n\n        title = f\"Prediction: {pred_texts[i]}\"\n        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n        ax[i // 4, i % 4].set_title(title)\n        ax[i // 4, i % 4].axis(\"off\")\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}